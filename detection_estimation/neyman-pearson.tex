%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}

\usepackage{xltxtra}
\usepackage{amsfonts}
\usepackage{polyglossia}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\geometry{a4paper,left=15mm,right=15mm,top=20mm,bottom=20mm}
\pagestyle{fancy}
\lhead{Devon Morris}
\chead{Detection \& Estimation Theory}
\rhead{\today}
\cfoot{\thepage}

\setlength{\headheight}{23pt}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.0in}

\newtheorem*{prop}{Proposition}
\newtheorem*{lem}{Lemma}
\newtheorem*{defn}{Definition}

\begin{document}
\section*{Neyman-Pearson Detection}%
Any yes-no question can be presented as a binary hypothesis. Here is some terminology. $\bm{X}$ is a random vector drawn from distribution $F_{\bm{\theta}}(\bm{X})$. Such that $\bm{\theta} \in \Theta$ and $\Theta = \Theta_0 \cup \Theta_1$, and we stipulate $\Theta_0 \cap \Theta_1 = \varnothing$. We want to test for the hypotheses, $H_0: \bm{\theta} \in \Theta_0$ and $H_1: \bm{\theta} \in \Theta_1$ and $\bm{\theta}$ is unknown. We first estimate $\bm{\theta}$ and then have a classification test. If $\Theta_i$ contains a single element then $H_i$ is a simple hypothesis, else $H_i$ is composite. So we define the detector
\[
  \phi(\bm{x}) = 
  \begin{cases}
    1 \sim H_1 & \bm{x} \in R \\
    0 \sim H_0 & \bm{x} \in A
  \end{cases}
\]
where $R,A \subset \mathds{R}^n$, and $R = A^c$. We need to define size, which is also known as $P_{FA}$, $\alpha$. 
\[
  \alpha := P_{\bm{\theta}_0}[\phi(\bm{x}) = 1]
\]
This is under the assumption that we have a simple hypothesis, we can also say
\[
  \alpha = E_{\bm{\theta}_0} \left[ \phi(\bm{x}) \right]
\]
If $H_0$ is composite, we take the highest $\alpha$.
\[
  \alpha = \sup_{\bm{\theta} \in \Theta_0} E_{\bm{\theta}_0} \left[ \phi(\bm{x}) \right]
\]
We define the power of a test
\[
  \beta := P_{\bm{\theta}_1} \left[ \phi(\bm{x}) = 1\right] = E_{\bm{\theta_1}} \left[ \phi(\bm{x}) \right]
\]
we want $\alpha$ to be zero, $\beta$ to be one. We can never get this. 

\subsection*{Receiver Operator Characteristic}%
This is a plot of $P_D$ vs $P_{FA}$. The line with slope 1 is the chance curve. We usually get some $\phi$ that thresholds a detection statistic. We will always call this the ROC curve. Just look this up in google for pictures.

\subsection*{Neyman-Pearson Detection}%
A Neyman-Pearson detector is given by
\[
  \phi(\bm{x}) = 
  \begin{cases}
    1 & f_{\bm{\theta}_1}(\bm{x}) > kf_{\bm{\theta}_0}(\bm{x}) \\
  \gamma & f_{\bm{\theta}_1}(\bm{x}) = k f_{\bm{\theta}_0}(\bm{x}) \\
    0 & f_{\bm{\theta}_1}(\bm{x}) < k f_{\bm{\theta}_0}(\bm{x}) \\
  \end{cases}
\]
with $k \geq 0$.

\begin{lem}[Neyman-Pearson Lemma]
  The N-P test $\phi(\bm{x})$ yields the most powerful test among all other of size $\alpha$. 
\end{lem}

\begin{proof}
  Let $\phi'(\bm{x})$ be some non-N-P test, such that $\alpha' \leq \alpha$. Consider 
  \[
    \int [\phi(\bm{x}) - \phi'(\bm{x})][f_{\bm{\theta}_1}(\bm{x}) - kf_{\bm{\theta}_0}(\bm{x})] \ dx = (\beta - \beta') + k(\alpha' - \alpha)
  \]
  Under $H_1$, $\bm{x} \in R$, we have that
  \[
    \begin{aligned}
      f_{\bm{\theta}_1} (\bm{x}) - k f_{\bm{\theta}_0} (\bm{x})  &> 0 \\
      \phi(\bm{x}) - \phi(\bm{x})  &\geq 0 \\
    \end{aligned}
  \]
  Under $H_0$, $\bm{x} \in A$, we have that
  \[
    \begin{aligned}
       f_{\bm{\theta}_1} (\bm{x}) - k f_{\bm{\theta}_0} (\bm{x})  &< 0 \\
       \phi(\bm{x}) - \phi(\bm{x})  &\leq 0 \\
    \end{aligned}
  \]
  So we have that
  \[
    \beta - \beta' \geq k(\alpha - \alpha')
  \]
  for $k \geq 0$. Therefore $\phi$ has power greater than or equal to the power of $\phi'$.
\end{proof}
Typically we set $\alpha$ and design $k$ based on $\alpha$.
\[
  \alpha = E_{\bm{\theta}_0} [\phi(\bm{x})] = 1 - P_{\bm{\theta}_0}[f_{\bm{\theta}_1}(\bm{x}) < k f_{\bm{\theta}_0}(\bm{x})] + \gamma P_{\bm{\theta}_0}[f_{\bm{\theta}_1}(\bm{x}) = k f_{\bm{\theta}_0}(\bm{x})]
\]
If the set of equality is a set of measure zero, then we just solve for $k$.

\subsubsection*{Example}%
Let $\bm{x} = \theta + \bm{\eta}$.  Under $H_0$, $\theta = -\mu$. Under $H_1$, $\theta = \mu$. $\eta$ has the pdf
\[
  f_{\eta} = 
  \begin{cases}
    1 - |\eta| & |\eta| \leq 1 \\
    0 & \text{else}
  \end{cases}
\]
Find $k$ for $N-P$ detector for $\alpha = 1/32$. Remember example from class with two triangles.

\begin{enumerate}
  \item Pick $\alpha$
  \item $x_{\alpha} = F^{-1}_{\bm{\theta}_0}(1 - \alpha)$
  \item $k = f_{\bm{\theta}_1}(\bm{x}_{\alpha})/f_{\bm{\theta}_0}(\bm{x}_{\alpha})$
\end{enumerate}

\subsection*{Neyman-Pearson Likelihood Ratio Test}%
We use the notation
\[
  \ell(\bm{x}) = \frac{f_{\bm{\theta}_1}(\bm{x})}{f_{\bm{\theta}_0}(\bm{x})}
\]
to define the likelihood ratio of two hypotheses. Note this is useful if this likelihood ratio is monotonically increasing. We will analyze Neyman-Pearson in the gaussian case. Specifically a binary hypothesis test with uncommon means and common variance.
\[
  \begin{aligned}
    H_0 &: \bm{x} \sim \mathcal{N} \left( \bm{m}_0, R \right) \\
    H_1 &: \bm{x} \sim \mathcal{N} \left( \bm{m}_1, R \right) \\
  \end{aligned}
\]
In this case we have the likelihood ratio of
\[
  \ell(\bm{x}) = \text{exp} \left( -\frac{1}{2}(\bm{x} - \bm{m}_1)^\top R^{-1} (\bm{x} - \bm{m}_1) + \frac{1}{2}(\bm{x} - \bm{m}_0)^\top R^{-1} (\bm{x} - \bm{m}_0)\right)
\] 
and we define the log likelihood
\[
  \mathcal{L}(\bm{x}) = \ln \left( \ell(\bm{x}) \right)= (\bm{m}_1 - \bm{m}_0)^\top R^{-1} \bm{x} + \frac{1}{2} (\bm{m}_0 + \bm{m}_0)^\top R^{-1}(\bm{m}_0 - \bm{m}_1)
\]
We can also rewrite it as
\[
  \mathcal{L}(\bm{x}) = (\bm{m}_1 - \bm{m}_0)^\top R^{-1} (\bm{x} - \bm{x_0})
\]
where $\bm{x}_0 = \frac{1}{2} (\bm{m}_1 + \bm{m}_0)$. This is a \underline{super simple} statistic. Let $\bm{w} := R^{-1}(\bm{m}_1 - \bm{m}_0)$ and we get $\mathcal{L}(\bm{x}) = \bm{w}^\top (\bm{x} - \bm{x}_0)$. This statistic is also a gaussian random variable and 
\[
  \mathcal{L}(X) \sim \mathcal{N} \left( - \frac{d^2}{2}, d^2 \right)
\]
where $d = \bm{w}^\top R \bm{w}$. We define a detector as
\[
  \phi(\bm{x}) = 
  \begin{cases}
    1 & \mathcal{L}(\bm{x}) > \eta \\
    0 & \mathcal{L}(\bm{x}) \leq \eta
  \end{cases}
\]
where $\eta = \ln(k)$.
We get that the size and power
\[
  \alpha = \int_{\eta}^{\infty} \frac{1}{(2\pi)^2 d} \text{exp} \left( -\frac{1}{2} (y + d/2)^2d^{-2} \right) = 1 - \Phi \left( \frac{\eta + d^2/2}{d} \right) = 1 - \Phi(z)
\]
where $z = (\eta + d^2/2)/2$, from this it is straightforward to show that
\[
  \beta = 1 - \Phi(z - d)
\]
Consider the binary system with source $\bm{s}$ where $\bm{s}$ goes through a mapper with outputs
\[
  \bm{m}_0 =
  \begin{bmatrix}
    m_0(0) \\
    \vdots \\
    m_{N-1}(0)
  \end{bmatrix}
  \quad 
  \bm{m}_1 =
  \begin{bmatrix}
    m_0(1) \\
    \vdots \\
    m_{N-1}(1)
  \end{bmatrix}
\]
and there is noise added to this signal so we have
\[
  \bm{x} = \bm{m} + \bm{n}
\]
We can define two hypotheses like
\[
  \begin{aligned}
    H_0: i = 0 \\
    H_1: i = 1
  \end{aligned}
\]
So we can form a detector
\[
  \phi(\bm{x}) = 
  \begin{cases}
    0 & \mathcal{L}(\bm{x}) \leq \eta \\
    1 & \mathcal{L}(\bm{x}) > \eta
    
  \end{cases}
\]
For this application we want $\alpha = 1 - \beta$. Under the special case where $\bm{m}_0^\top R^{-1} \bm{m}_0 = \bm{m}_1^\top R^{-1} \bm{m}_1 = E_s$.

\end{document}
