%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}

\usepackage{xltxtra}
\usepackage{amsfonts}
\usepackage{polyglossia}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{float}
\usepackage{tikz}
\usepackage{bm}
\usepackage[shortlabels]{enumitem}
\usetikzlibrary{shapes,arrows,positioning}

\geometry{a4paper,left=15mm,right=15mm,top=20mm,bottom=20mm}
\pagestyle{fancy} \lhead{Devon Morris}
\chead{Detection \& Estimation Theory - Homework 6}
\rhead{\today}
\cfoot{\thepage}

\setlength{\headheight}{23pt}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.0in}

\newtheorem{prop}{Proposition}
\newtheorem*{sol}{Solution}

\tikzset{
block/.style = {draw, fill=white, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate}, 
sum/.style= {draw, fill=white, circle, node distance=1cm},
input/.style = {coordinate},
output/.style= {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}
}
}

\begin{document}
\section*{Problem 1}%

Suppose that signal $\bm{s}$ constains the elements $s_n = A\cos (n\omega - \phi)$ where $\omega$ is an unknown angular frequency, $A$ is an unknown amplitude, and $\phi$ is an unknown phase. Show that $\bm{s}$ has the representation
\[
  \bm{s} = \bm{H}\bm{\theta}
\]
\[
  \bm{H} = 
  \begin{bmatrix}
    1 & 0 \\
    \cos \omega & \sin \omega \\
    \vdots & \vdots \\
    \cos (N-1)\omega & \sin(N-1) \omega
  \end{bmatrix}; \quad
  \bm{\theta} = 
  \begin{bmatrix}
    A \cos \phi \\
    A \sin \phi
  \end{bmatrix}
\]
Thus a sinusoidal signal with unknown amplitude and phase lies in a two-dimensional linear subspace spanned by the cosinusoidal and sinusoidal components of the matrix $\bm{H}$. These are often called inphase and quadrature components. Let $\bm{x} = \mu \bm{s} + \bm{n}$ and assume $\bm{n} \sim \mathcal{N}(0, \sigma^2I)$. Find the uniformly most powerful tests of $H_0: \mu = 0$ versus $H_1: \mu > 0$ when $\sigma^2$ is known and when $\sigma^2$ is unknown. Defend any invariance requirements and write out all of your quadratic forms to illustrate what is going on. Characterize the performance for each case.

\begin{sol}
  Multiplying $\bm{H}$ and $\bm{\theta}$ and applying the product sum trig identities, we get 
  \[
    \begin{aligned}
      \bm{H}\bm{\theta} &=
  \begin{bmatrix}
    1 & 0 \\
    \cos \omega & \sin \omega \\
    \vdots & \vdots \\
    \cos (N-1)\omega & \sin(N-1) \omega
  \end{bmatrix}
  \begin{bmatrix}
    A \cos \phi \\
    A \sin \phi
  \end{bmatrix} \\
                        &= \begin{bmatrix}
    A \cos \phi \\
    A\cos \phi \cos \omega + A \sin \phi \sin \omega \\
    \vdots \\
    A\cos \phi \cos(N-1)\omega + A\sin \phi \sin (N-1)\omega
  \end{bmatrix} \\
                        &=
  \begin{bmatrix}
    A \cos \phi \\
    A \cos (\omega - \phi) \\
    \vdots \\
    A \cos ((N-1)\omega - \phi)
  \end{bmatrix}
    \end{aligned}
  \]
  At this point, we note that $\bm{x} \sim \mathcal{N}(\mu \bm{s}, \sigma^2 I)$. Now let us suppose that $\sigma^2$ is known. Thus in this case, we have that $\theta = \mu$ and we have 
  \[
    \begin{aligned}
      f_{\theta}(\bm{x}) &= (2\pi)^{-N/2} \sigma^N \exp \left( -\frac{1}{2\sigma^2} (\bm{x} - \mu \bm{s})^\top (\bm{x} - \mu \bm{s}) \right) \\
                         &=  (2\pi)^{-N/2} \sigma^N  \exp \left( -\frac{1}{2\sigma^2} \bm{x}^\top\bm{x} \right) \exp \left( -\frac{\mu^2}{2\sigma^2} \bm{s}^\top \bm{s}  \right) \exp \left( \frac{\mu}{2\sigma^2} \bm{s}^\top \bm{x} \right)
    \end{aligned}
  \]
  So by Fisher-Neyman we have that
  \[
    T(\bm{x}) = \frac{1}{2\sigma^2}\bm{s}^\top \bm{x}
  \]
  is sufficient for $\mu$. Furthermore, we have that $t \sim \mathcal{N}(\mu \bm{s}^\top \bm{s}/2\sigma^2, \bm{s}^\top \bm{s}/4\sigma^2)$. So we have that the likelihood ratio test $\ell(t)$ is a non-decreasing function of $t$ since
  \[
    L(t) = \frac{4\sigma^2}{\bm{s}^\top\bm{s}}\left(t\mu_1\frac{\bm{s}^\top \bm{s}}{2\sigma^2}\right) = \frac{1}{2} t\mu_1
  \]
  where $\mu_1 > 0$ therefore, the test 
  \[
    \phi(t) 
    = 
    \begin{cases}
      1, & t > t_0 \\
      0, & t \leq t_0
    \end{cases}
  \]
  is uniformly most powerful (UMP) of its size (Karlin-Rubin). So given a $t_0$ we have that
  \[
    \alpha = E_{\theta_0} \phi(t) = P_{\theta_0}[t > t_0] = 1 - P_{\theta_0}[t \leq t_0] = 1 - \Phi_n(4\sigma^2 t_0/\bm{s}^\top\bm{s})
  \]
  Note that we do not have to take a $\sup$ because the null hyptothesis is simple.
  
  Now assume that $\sigma^2$ is unknown. Therefore $\theta = (\mu, \sigma^2)$ and $\sigma^2$ is a nuissance parameter. Consider the statistics given by
  \[
    \begin{aligned}
      t_1 &= \bm{s}^\top \bm{x} \\
      t_2 &= \bm{x}^\top \bm{x}
    \end{aligned}
  \]
  We note that these statistics are sufficient for $(\mu, \sigma^2)$. We note that the transformation $g(\bm{x}) = c\bm{x}$, $c > 0$ leaves the hypothesis space invariant because under the null hypothesis we are still mean zero and under hypothesis $1$ the mean is still greater than $0$. So now lets consider the statistic 
  \[
    t = \frac{\bm{s}^\top \bm{x}}{(\bm{x}^\top \bm{x})^{1/2}}
  \]
  This statistic is invariant under the data transformation $g$. Furthermore, it is $t$-distributed and thus will have a monotone likelihood ratio.  So now we can use the test
  \[
    \phi(t) 
    = 
    \begin{cases}
      1, & t > t_0 \\
      0, & t \leq t_0
    \end{cases}
  \]
  and we have a UMP invariant test. For a given $\alpha$ it will have lower $\beta(\mu)$ across the alternative hypothesis set. This is due to having less discriminating information.
\end{sol}

\section*{Problem 2}%
Let $\delta \bm{s}$ denote a known signal whose polarity is switching by $\delta = \pm 1$. Consider the test $H_0: \bm{x} \sim \mathcal{N}(\bm{0}, \sigma^2 I)$ versus $H_1: \bm{x} \sim \mathcal{N}(\mu\delta \bm{s}, \sigma^2 I)$, $\mu > 0$. For each of the following cases, determine an appropriate invariance condition and derive a UMP-invariant detector.
\begin{enumerate}[a.]
  \item $\sigma^2$ known, $\delta = 1$;
  \item $\sigma^2$ known, $\delta = \pm 1$;
  \item $\sigma^2$ unknown, $\delta = 1$;
  \item $\sigma^2$ unknown, $\delta = \pm 1$.
\end{enumerate}


\begin{sol}
  \begin{enumerate}[a.]
    \item We note that the statistic given by $t = \bm{s}^\top \bm{x}$ is sufficient for $\mu$. Furthermore $t \sim \mathcal{N}(\mu \delta \bm{s}^\top \bm{s}, \sigma^2 \bm{s}^\top \bm{s})$. As shown previously, the likelihood ratio of $t$ is monotonically increasing. So the test
  \[
    \phi(t) 
    = 
    \begin{cases}
      1, & t > t_0 \\
      0, & t \leq t_0
    \end{cases}
  \]
  is UMP. No invariance condition is required.
\item We note that the statistic $t = \bm{s}^\top \bm{x}$ is sufficient for $\mu$, however it is also sufficient for $\delta$, by Fisher-Neyman. Now we consider the statistic $t = (\bm{s}^\top \bm{x})^2/\sigma^2$. We note that this statistic is invariant under transformations $g(\bm{x}) = c \bm{x}$, $c = \pm 1$.  Furthermore, we have that $t$ is $\chi^2$ and thus has monotone likelihood ratio. So the test
  \[
    \phi(t) 
    = 
    \begin{cases}
      1, & t > t_0 \\
      0, & t \leq t_0
    \end{cases}
  \]
  is UMP.
\item This case is the same as Problem 1.
\item We consider the statistic given by
  \[
    t = \frac{(\bm{s}^\top \bm{x})^2}{(\bm{x}^\top \bm{x})^{1/2}}
  \]
  This statistic is invariant to changes in $\delta$ and $\sigma$, which is $\chi$ distributed. This also has monotone likehlihood ratio and therefore
  \[
    \phi(t) 
    = 
    \begin{cases}
      1, & t > t_0 \\
      0, & t \leq t_0
    \end{cases}
  \]
  is UMP.
  \end{enumerate}  
\end{sol}

\section*{Problem 3}%
Let $X = (X_1, \dots, X_M)$ denote a sample of $U(0,\theta)$ random varaibles. Show that
\begin{enumerate}[a.]
  \item $\hat{\theta}_{ML} = \max_m X_m$;
  \item The density of $\hat{\theta}_{ML}$ is $f_{\hat{\theta}}(x) = \frac{M}{\theta^M} x^{M-1}$, $0 \leq x \leq \theta$.
  \item $E \hat{\theta}_{ML} = \frac{M}{M+1} \theta$
  \item $\text{var} (\hat{\theta}_{ML}) = \frac{M}{(M+2)(M+1)^2} \theta^2$
\end{enumerate}

\begin{sol}
  \begin{enumerate}[a.]
    \item Consider the likelihood $L(\theta, X)$
  \[
    L(\theta, X) = \frac{1}{\theta^M} \bm{1}_{[\min x_i, \theta]} (\max x_i) = \frac{1}{\theta^M} \bm{1}_{[\max x_i, \infty)} (\theta)
  \]
  The likelihood is therefore maximized when
  \[
    \hat{\theta}_{ML} = \max_i x_i
  \]
    \item Consider the probability
      \[
        F_{\hat{\theta}}(x) = P[\hat{\theta}_{ML} \leq x] =  P[ X_i \leq x, \forall x_i] = \frac{x^M}{\theta^M}, 0 \leq x \leq \theta
      \]
      Now taking the derivative we have
      \[
        f_{\hat{\theta}}(x) = \frac{M}{\theta^M} x^{M-1}, 0 \leq x \leq \theta
      \]
    \item Consider the expectation
      \[
        E \hat{\theta}_{ML} =  \frac{M}{\theta^M} \int_{0}^{\theta} x^M\ dx = \frac{M}{M+1} \frac{1}{\theta^M}\eval{x^{M+1}}_0^\theta = \frac{M}{M+1} \theta
      \]
    \item First let's calculate 
      \[
        E \hat{\theta}_{ML}^2 = \frac{M}{\theta^M} \int_{0}^\theta x^{M+1}\ dx = \frac{M}{M+2} \frac{1}{\theta^M} \eval{x^{M+2}}_0^\theta = \frac{M}{M+2} \theta^2 =  \frac{M(M+1)^2 }{(M+2)(M+1)^2} \theta^2
      \]
      We also have that
      \[
        (E\theta_{ML})^2 = \frac{M^2}{(M+1)^2} \theta^2 = \frac{M^2(M+2)}{(M+2)(M+1)^2}\theta^2
      \]
      So by simple calculation we have that
      \[
        \text{var}(\hat{\theta}_{ML}) = \frac{M}{(M+2)(M+1)^2} \theta^2
      \]
  \end{enumerate}
\end{sol}

\section*{Problem 4}%
Let $\left\{\bm{X}_i  \right\}_0^{M-1}$ denote a sequence of random vectors with mean $\bm{m}(\bm{\theta})$ and covariance $R$. Define the matrix
\[
  \bm{S} = \frac{1}{M} \sum_{i=0}^{M-1} (\bm{X_i} - \bm{m})(\bm{X}_i - \bm{m})^\top
\]
Show
\begin{enumerate}[a.]
  \item $E \bm{S} = R; \bm{R} = E (\bm{X}_i - \bm{m})(\bm{X}_i - \bm{m})^\top$; 
  \item $E \pdv{\bm{S}}{\theta_n} = 0$
  \item $E \pdv{\bm{S}}{\theta_n}{\theta_k} = \pdv{\bm{m}}{\theta_k} \pdv{\bm{m}^\top}{\theta_n} + \pdv{\bm{m}}{\theta_n} \pdv{\bm{m}^\top}{\theta_k}$
\end{enumerate}

\begin{sol}
  \begin{enumerate}[a.]
   \item By linearity, we have that
     \[
       E \bm{S} = \frac{1}{M} \sum_{i=0}^{M-1} E (\bm{X}_i - \bm{m})(\bm{X}_i -\bm{m})^\top = \frac{M}{M} R = R
     \]
   \item First consider $\pdv{\bm{S}}{\theta_n}$
     \[
       \begin{aligned}
         \pdv{\bm{S}}{\theta_n} &=  \frac{1}{M} \sum_{i=0}^{M-1}  - \left(\pdv{\bm{m}}{\theta_n} \right)(\bm{X}_i - \bm{m})^\top - (\bm{X}_i - \bm{m})\left( \pdv{\bm{m}}{\theta_n} \right)^\top
       \end{aligned}
     \]
     From this, it is easy to see that 
     \[
       E  \pdv{\bm{S}}{\theta_n} = \bm{0}
     \]
   \item Now we have that 
     \[
       \pdv{\bm{S}}{\theta_n}{\theta_k} = \frac{1}{M} \sum_{i=1}^{M-1}  -\left( \pdv{\bm{m}}{\theta_n}{\theta_k} \right)(\bm{X}_i - \bm{m})^\top - (\bm{X}_i - \bm{m}) \left( \pdv{\bm{m}}{\theta_n}{\theta_k} \right) + \pdv{\bm{m}}{\theta_n} \pdv{\bm{m}}{\theta_k}^\top + \pdv{\bm{m}}{\theta_k}\pdv{\bm{m}}{\theta_n}^\top 
     \]
     So taking the expectation yields
     \[
       E\pdv{\bm{S}}{\theta_n}{\theta_k} = \pdv{\bm{m}}{\theta_n} \pdv{\bm{m}}{\theta_k}^\top + \pdv{\bm{m}}{\theta_k}\pdv{\bm{m}}{\theta_n}^\top 
     \]
 \end{enumerate} 
\end{sol}

\section*{Problem 5}%
Let $\left\{ \bm{x}_i \right\}_0^{M-1}$ denote a random sample of $\mathcal{N}(0,R)$ random vectors. Find the ML estimate of $\sigma^2$ in the covariance model
\[
  R = \sigma^2 U_p \Sigma^2_p U_p^\top; \quad (U_p, \Sigma_p^2) \text{ known }
\]
Compute the Fisher information matrix. Is the ML estimate unbiased? minimum variance? efficient? Draw a block diagram for $\hat{\sigma^2}$ and interpret your result.

\begin{sol}
 It is straightforward to show that 
 \[
   L(\theta, \bm{X}) = -\frac{MN}{2} \ln (\pi) - \frac{MN}{2}\ln(\sigma^2) - \frac{N}{2} \ln (\det (\Sigma_p^2)) - \frac{1}{2\sigma^2} \sum_{i=0}^{M-1} \bm{x}_i^\top U_p \Sigma_p^{-2} U_p^\top \bm{x}_i
 \]
 taking the derivative and equating it to zero gives us
 \[
   -\frac{MN}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=0}^{M-1} \bm{x}_i^\top U_p \Sigma_p^{-2} U_p^\top \bm{x}_i = 0
 \]
 which implies that
 \[
   \hat{\sigma}^2_{ML} = \frac{1}{MN} \sum_{i=1}^{M-1}\bm{x}_i^\top U_p \Sigma_p^{-2} U_p^\top \bm{x}_i
 \]
 In this case, since there is only one parameter, the fisher information matrix is just a scalar. Taking the second derivative of the likelihood gives us
 \[
   J(\theta, \bm{X}) = - \frac{MN}{2 (\sigma^2)^2} + \frac{1}{(\sigma^2)^3}\sum_{i=1}^{M-1}\bm{x}_i^\top U_p \Sigma_p^{-2} U_p^\top \bm{x}_i
 \]
 now we need to take the expectation. At this point we note that that $\Sigma_p^{-1}U_p^\top$ is in a sense whitening the data, so we have
 \[
   J = -\frac{MN}{2(\sigma^2)^2} + \frac{MN\sigma^2}{(\sigma^2)^3} = \frac{MN}{2(\sigma^2)^2}
 \]
 We also note that $\hat{\sigma}^2_{ML}$ is unbiased by a similar calculation
 \[
   E \hat{\sigma}^2_{ML} = \sigma^2
 \]
 Let us check the efficiency. By the efficiency theorem we have
 \[
   \frac{MN}{2\sigma^4}(\sigma^2_{ML} - \sigma^2) = -\frac{MN}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=0}^{M-1} \bm{x}_i^\top U_p \Sigma_p^{-2} U_p^\top \bm{x}_i 
 \]
 which is precisely our score function. So our estimator is efficient which implies that it is also minimum variance.
\end{sol}

\section*{Problem 6}%
Let $X = (X_1, \dots, X_N)$ denote a random sample of exponential random variables with unknown paramater $\theta$:
\[
  f_{\theta}(x_n) = \frac{1}{\theta} e^{-x_n/\theta}
\]
Compute the fisher information matrix. Is the ML estimate unbiased? minimum variance? efficient?

\begin{sol}
  The log-likelihood function is given by
  \[
    L(\theta, X) = -N \ln \theta - \frac{1}{\theta} \sum_{i=1}^N x_i
  \]
  the score function is therefore
  \[
    s(\theta, X) = -\frac{N}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^N x_i
  \]
  So 
  \[
    \hat{\theta}_{ML} = \frac{1}{N} \sum_{i=1}^N x_i
  \]
  Now we will take the derivative of our score function
  \[
    \pdv{\theta} s(\theta, X) = \frac{N}{\theta^2} - \frac{2}{\theta^3} \sum_{i=1}^N x_i
  \]
  which implies that 
  \[
    J = -\frac{N}{\theta^2} + \frac{2N}{\theta^2} = \frac{N}{\theta^2}
  \]
  It is easy to see that $\hat{\theta}_{ML}$ is unbiased. Computing the variance, we have
  \[
    E(\hat{\theta} - \theta)^2 = \theta^2 - 2 \theta^2  + \frac{1}{N^2} \sum_{i=1}^N E x_i^2 = -\theta^2  + \frac{\theta^2}{N}
  \]
\end{sol}


\end{document}
