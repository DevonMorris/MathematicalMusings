%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}

\usepackage{xltxtra}
\usepackage{amsfonts}
\usepackage{polyglossia}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}

\geometry{a4paper,left=15mm,right=15mm,top=20mm,bottom=20mm}
\pagestyle{fancy}
\lhead{Devon Morris}
\chead{Detection \& Estimation Theory}
\rhead{\today}
\cfoot{\thepage}

\setlength{\headheight}{23pt}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.0in}

\newtheorem{prop}{Proposition}
\newtheorem*{defn}{Definition}

\begin{document}

\section*{Sufficiency}
\begin{enumerate}
  \item $\mathbf{x}$ are measurements (samples) from a distribution $F_{\bm{\theta}}(\mathbf{x})$
  \item $\bm{\theta}$ is a vector of parameters definining $F_{\bm{\theta}}(\mathbf{x})$
  \item Is there a function $\mathbf{t} = T(\mathbf{x})$ which  retains all useful information in $\mathbf{x}$ with respect to $\bm{\theta}$.
  \item We want $T(\mathbf{x})$ to be a "sufficient" statistic.
\end{enumerate}
What we want is a length $\mathbf{t} \ll$ length $\mathbf{x}$. If we can find such a $T$ we have found a sufficient statistic. 

\begin{defn}[Sufficient Statistic]
  Let $\mathbf{X}$ be a length $N$ random vector. Let $\bm{\theta}$ be a $p$-dimesional, parameter vector such that $P_{\bm{\theta}}(\mathbf{x})$ is the probability mass function for $\mathbf{X}$, the statistic $T_{\mathbf{x}}$ is sufficient for $\bm{\theta}$ if $P_{\bm{\theta}}(\mathbf{x} | T(\mathbf{x}) = \mathbf{t})$ is independent of $\bm{\theta}$. I.e.  $P_{\bm{\theta}}(\mathbf{x} | T(\mathbf{x}) = \mathbf{t}) = P(\mathbf{x} | \mathbf{t})$
\end{defn}
The continuous definition is the same, but exchange $P_{\bm{\theta}}$ for $f_{\bm{\theta}}$.
Consider $P_{\bm{\theta}}(\mathbf{x}, \mathbf{t})$. We have that
\[
P_{\bm{\theta}}(\mathbf{x}, \mathbf{t}) = P_{\bm{\theta}}(X = \mathbf{x}, T(X) = \mathbf{t}) = 
\begin{cases}
  P_{\bm{\theta}}(\mathbf{x}) & \mathbf{t} = T(\mathbf{x}) \\
  0 & \mathbf{t} \neq T(\mathbf{x})
\end{cases}
\]
Let apply the definition of conditional probability and we get
\[
  P_{\bm{\theta}}(\mathbf{x}|\mathbf{t})P_{\bm{\theta}}(\mathbf{t}) = P_{\bm{\theta}}(\mathbf{x}, \mathbf{t})
\]
Or equivalently, we can write
\[
  P_{\bm{\theta}}(\mathbf{x}) =  
\begin{cases}
  P_{\bm{\theta}}(\mathbf{x}|\mathbf{t})P_{\bm{\theta}}(\mathbf{t})  & \mathbf{t} = T(\mathbf{x}) \\
  0 & \mathbf{t} \neq T(\mathbf{x})
\end{cases}
\]
So if $T(\mathbf{x})$ is sufficient we have 
\[
  P_{\bm{\theta}}(\mathbf{x}) =  
\begin{cases}
  P(\mathbf{x}|\mathbf{t})P_{\bm{\theta}}(\mathbf{t})  & \mathbf{t} = T(\mathbf{x}) \\
  0 & \mathbf{t} \neq T(\mathbf{x})
\end{cases}
\]

\subsection*{Example}%
Let $X = [X_1, \dots, X_N]$, where each $X_i$ is poisson.
\[
  P_{\theta}(x) = e^{-\theta} \frac{\theta^x}{x!}
\]
$\theta$ is the expected arrivals in a unit time period. So we have $E[X] = \theta$. We need a $P_{\theta}(\mathbf{x})$.
\[
  P_{\theta}(\mathbf{x}) = \prod_{n=1}^N P_{\theta}(x_n) = e^{-N\theta}  \frac{\theta^{\sum_{n=1}^N x_n}}{\prod_{n=1}^N x_n!}
\]
Let $k = T(\mathbf{x}) = \sum_{n=1}^N x_n$. 
\[
  \begin{aligned}
    P_{\theta}(\mathbf{x}) &= \frac{1}{\prod_{n=1}^N x_n!} e^{-N \theta} \theta^k \\
  \end{aligned}
\]
So we have succesfully factored this into a term that only depends on the data, and a term that only depends on the parameter $\theta$.

If we analyze, 
\[
  \frac{P_{\theta}(\mathbf{x})}{P_{\theta}(k)} = \frac{k! N^{-k}}{\prod_{n=1}^N x_i!}
\]

we see that it does not depend on $\theta$. Therefore, we have a sufficient statistic. For continuous random variables, we just replace $P$ with $f$.

\subsection*{Example}%
Let $X = [X_0: \dots: X_{m-1}]$. Where each $X_n$ is independent of $X_m$ and $X_i \sim \mathcal{N}(m, R)$
we get
\[
  f_{\bm{\theta}}(\mathbf{X}) = 
  (2\pi)^{-MN/2} \text{det}(R)^{-M/2} \text{exp} \left( -\frac{M}{2} \text{tr} \left( R^{-1} \mathcal{S}(\mathbf{m}) \right)\right)
\]
where 
\[
  \mathcal{S}(\mathbf{m}) = \frac{1}{M} \sum_{m=0}^{M-1} (\mathbf{x}_m  - \mathbf{m})(\mathbf{x}_m - \mathbf{m})^\top
\]
We want to find a sufficient set of statistics. We will define a sample mean and sample covariance
\[
  \hat{\bm{m}} = \frac{1}{M}\sum_{n=0}^{M-1} \bm{x}_n
\]
and 
\[
  \hat{\mathcal{S}}(\hat{\bm{m}}) = \frac{1}{M}  \sum_{n=1}^{M-1} (\bm{x}_n - \hat{\bm{m}})(\bm{x}_n - \hat{\bm{m}})^\top
\]
Adding and subtracing the sample mean gives 
\[
  \begin{aligned}
    f_{\bm{\theta}}(\mathbf{X}) &= 
  (2\pi)^{-MN/2} \text{det}(R)^{-M/2} \text{exp} \left( -\frac{1}{2} \sum (\bm{x}_n - \hat{\bm{m}} + \hat{\bm{m}} - \bm{m})^\top R^{-1}(\bm{x}_n - \hat{\bm{m}} + \hat{\bm{m}} - \bm{m})\right) \\
                                &=  a\text{det}(R)^{-M/2} \text{exp} \left( -\frac{M}{2} \text{tr} \left( R^{-1}(\hat{\bm{m}} - \bm{m})(\hat{\bm{m}} - \bm{m}\right) \right) \text{exp} \left( -\frac{M}{2} \text{tr} \left( R^{-1}\hat{\mathcal{S}}(\hat{\bm{m}}) \right) \right)
  \end{aligned}
\]
if we let 
\[
  b_{\bm{\theta}}(\hat{\bm{m}}, \hat{\mathcal{S}}(\hat{\bm{m}})) = \text{det}(R)^{-M/2} \text{exp} \left( -\frac{M}{2} \text{tr} \left( R^{-1}(\hat{\bm{m}} - \bm{m})(\hat{\bm{m}} - \bm{m}\right) \right) \text{exp} \left( -\frac{M}{2} \text{tr} \left( R^{-1}\hat{\mathcal{S}}(\hat{\bm{m}}) \right) \right)
\]
so
\[
  f_{\bm{\theta}} = a(\bm{x})  b_{\bm{\theta}}(\hat{\bm{m}}, \hat{\mathcal{S}}(\hat{\bm{m}})) 
\]
So we have that
\[
  T(\bm{x}) = \left\{ \hat{\bm{x}}, \hat{\mathcal{S}}(\hat{\bm{m}}) \right\}
\]

\section*{Minimal and Complete Sufficient Statistics}%
A sufficient statistic $T(\mathbf{x}) = \bm{t}$ is minimal if it can be represented as a function of all other sufficient (for $\bm{\theta}$) statistics on $\bm{x}$. A sufficient statistic $T(\mathbf{x})$ is complete or (unique) if the condition
\[
  E_{\bm{\theta}} \left[ W(T(\mathbf{x})) \right] = 0
\]
for all $\bm{\theta}$ implies $W(T(\bm{x})) = 0$ with probability 1. We note that completeness implies minimality.

\section*{Unbiasedness}%
Just one function $W$ of a complete statistic produces an unbiased estimate of $\bm{\theta}$.

\section*{Strategy}
\begin{enumerate}
  \item Sufficient Statistic (Fisher-Neyman) factorization theorem to identify a sufficient statistic.
  \item Check for completeness, which gives us minimality.
  \item This statistic is lowest order
  \item If complete, find $W$ such that $\hat{\bm{\theta}} = W(T(\mathbf{x}))$
\end{enumerate}

\end{document}
